---
title: "Practical Machine Learning Course Project"
author: "Gregory Kanevsky"
date: "Sunday, January 25, 2015"
output: html_document
---

<!-- rmarkdown v1 -->

Data
=========
After downloading data files to local storage they are loaded as follows:
```{r echo=FALSE, cache=TRUE}
trainingFileDir = "C:/Users/GK186006/Dropbox/Coursera/Practical Machine Learning/Course Projecct and Quizes/data/pml-training.csv"
testingFileDir = "C:/Users/GK186006/Dropbox/Coursera/Practical Machine Learning/Course Projecct and Quizes/data/pml-testing.csv"
```
```{r Loading Data, cache=TRUE}
trainData = read.csv(trainingFileDir, stringsAsFactors=FALSE)
testData = read.csv(testingFileDir, stringsAsFactors=FALSE)
```

Libraries Used
========
During course of the project following libraries used:
```{r Libraries, cache=TRUE, warning=FALSE}
library(caret)
library(plyr)
library(reshape2)
library(Hmisc)
library(corrplot)
library(ggplot2)
```

Removing incomplete attributes and preping data
=======
Original data set contains `r dim(trainData)[[1]]` data rows and `r dim(trainData)[[2]]` columns.
Among them first 7 columns and last column are non-character, while the rest are numeric. Due to missing
data not all data loaded in numeric format. We convert them to numeric as follows:
```{r Conversion to numeric format, cache=TRUE, warning=FALSE}
nonNumCols = c(-1:-7, -ncol(trainData))
trainData[,nonNumCols] = apply(trainData[,nonNumCols], 2, function(x) as.numeric(x))
testData[,nonNumCols] = apply(testData[,nonNumCols], 2, function(x) as.numeric(x))

```

Choosing attributes (predictors)
=======
Reducing number of predictors without sacraficing quality is the goal of this exercise.
Firstly, let's remove predictors that contain 1% or greater of NAs:
```{r Remove NAs, cache=TRUE}
N = dim(trainData)[[1]]
naCounts = sapply(trainData, FUN=function(x) sum(is.na(x)))
names01 = names(which(naCounts <= 0.01 * N))
trainData = trainData[, names01]
testData = testData[, c(names01[-length(names01)], "problem_id")]

```

Lastly, let's remove highly correlated predictors.
```{r cache=TRUE, echo=FALSE}
flattenCorrMatrix <- function(cormat, pmat, stringsAsFactors=FALSE) {
  ut = upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut],
    stringsAsFactors = stringsAsFactors
  )
}
```

```{r Correlation Matrix, cache=TRUE}
corrMatrix = rcorr(as.matrix(trainData[,names01[c(-1:-7,-length(names01))]]))
corrplot(corrMatrix$r, type="upper", order="hclust", tl.col="black", tl.srt=45)

```

```{r Remove Highly Correlated Predictors, cache=TRUE}
# Remove highly correlated columns (cor > threshold)
threshold = 0.75
flattenCorrMatrix = flattenCorrMatrix(corrMatrix$r, corrMatrix$P)
colsToRemove = unique(flattenCorrMatrix[abs(flattenCorrMatrix$cor) >= threshold, 'column'])
trainData = trainData[, setdiff(names(trainData), colsToRemove)]

```


Partition data
==========
```{r Partition Data, cache=TRUE}
# Partition data
inTrain = createDataPartition(y=trainData$classe, p=0.75, list=FALSE)
training = trainData[inTrain, -1:-7]
testing = trainData[-inTrain, -1:-7]
```



```{r Random Forest with PCA, cache=TRUE}
preProc = preProcess(training[,-ncol(training)], method="pca")
trainingPCA = predict(preProc, training[,-ncol(training)])
modelFitPCA = train(factor(training$classe) ~ ., method="rf", data=trainingPCA)
testingPCA = predict(preProc, testing[,-ncol(testing)])
```

```{r Random Forest with PCA Test, cache=TRUE}
confusionMatrix(factor(testing$classe), predict(modelFitPCA, testingPCA))
```

```{r Random Forest, cache=TRUE}
modelFitRF = train(factor(training$classe) ~ ., method="rf", data=training)
```

```{r Random Forest Test, cache=TRUE}
confusionMatrix(factor(testing$classe), predict(modelFitRF, testing))
```







---
title: "Practical Machine Learning Course Project"
author: "Gregory Kanevsky"
date: "Sunday, January 25, 2015"
output: html_document
---

<!-- rmarkdown v1 -->

Data
=========
After downloading data files to local storage they are loaded as follows:
```{r echo=FALSE, cache=TRUE}
trainingFileDir = "C:/Users/GK186006/Dropbox/Coursera/Practical Machine Learning/Course Projecct and Quizes/data/pml-training.csv"
testingFileDir = "C:/Users/GK186006/Dropbox/Coursera/Practical Machine Learning/Course Projecct and Quizes/data/pml-testing.csv"
```
```{r Loading Data, cache=TRUE}
trainData = read.csv(trainingFileDir, stringsAsFactors=FALSE)
testData = read.csv(testingFileDir, stringsAsFactors=FALSE)
```

Libraries Used
========
During course of the project following libraries used:
```{r Libraries, cache=TRUE, warning=FALSE}
library(caret)
library(plyr)
library(reshape2)
library(Hmisc)
library(corrplot)
library(ggplot2)
```

Removing incomplete attributes and preping data
=======
Original data set contains `r dim(trainData)[[1]]` data rows and `r dim(trainData)[[2]]` columns.
Among them first 7 columns and last column are non-character, while the rest are numeric. Due to missing
data not all data loaded in numeric format. We convert them to numeric as follows:
```{r Conversion to numeric format, cache=TRUE, warning=FALSE}
nonNumCols = c(-1:-7, -ncol(trainData))
trainData[,nonNumCols] = apply(trainData[,nonNumCols], 2, function(x) as.numeric(x))
testData[,nonNumCols] = apply(testData[,nonNumCols], 2, function(x) as.numeric(x))

```

Choosing attributes (predictors)
=======
Reducing number of predictors without sacraficing quality is the goal of this exercise.
Firstly, let's remove predictors that contain 1% or greater of NA values:
```{r Remove NAs, cache=TRUE}
N = dim(trainData)[[1]]
naCounts = sapply(trainData, FUN=function(x) sum(is.na(x)))
names01 = names(which(naCounts <= 0.01 * N))
trainData = trainData[, names01]
testData = testData[, c(names01[-length(names01)], "problem_id")]

```

Lastly, let's remove highly correlated predictors. If correlation between predictors
is above threshold of 0.75 (absolute value) then we remove 2d predictor as reduntant.
```{r cache=TRUE, echo=FALSE}
flattenCorrMatrix <- function(cormat, pmat, stringsAsFactors=FALSE) {
  ut = upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut],
    stringsAsFactors = stringsAsFactors
  )
}
```

This is illustration of correlation relationships between predictors:
```{r Correlation Matrix, cache=TRUE}
corrMatrix = rcorr(as.matrix(trainData[,names01[c(-1:-7,-length(names01))]]))
corrplot(corrMatrix$r, type="upper", order="hclust", tl.col="black", tl.srt=45)

```

```{r Remove Highly Correlated Predictors, cache=TRUE}
# Remove highly correlated columns (cor > threshold)
threshold = 0.75
flattenCorrMatrix = flattenCorrMatrix(corrMatrix$r, corrMatrix$P)
colsToRemove = unique(flattenCorrMatrix[abs(flattenCorrMatrix$cor) >= threshold, 'column'])
trainData = trainData[, setdiff(names(trainData), colsToRemove)]

```
Plotting each predictor left by classes (colors). To make plot more manageable only 10% training
data displayed in plots (sampled randomly with equal class representation): 
```{r Plotting predictors, cache=TRUE, echo=FALSE}
# Sample function
sampleByClasse <- function(df, percent=0.10) {
  n = percent * nrow(df)
  ddply(df, .(classe), function(x) x[sample(nrow(x), size=n),])
}


# Visualize on sample
reviewData = sampleByClasse(trainData)
reviewData$point = as.integer(rownames(reviewData))
reviewData = melt(reviewData[, -1:-7], id.vars=c("point", "classe"))
ggplot(reviewData) + 
  geom_point(aes(point, value, colour=classe)) + facet_wrap(~variable, scales="free_y") +
  guides(colour=FALSE)


```


Partition data
==========
Dividing data set into training and testing will alow us to validate resulting model
and estimate its out of sample error:
```{r Partition Data, cache=TRUE}
# Partition data
inTrain = createDataPartition(y=trainData$classe, p=0.75, list=FALSE)
training = trainData[inTrain, -1:-7]
testing = trainData[-inTrain, -1:-7]
```

Random Forest Model with PCA
==========
Building Random Forest with pre-processing data using PCA attempts further 
reduce number of predictors while accounting for majority of variability in data
(using PCA). Random Forest model uses default configuration in caret:
```{r Random Forest with PCA, cache=TRUE}
preProc = preProcess(training[,-ncol(training)], method="pca")
trainingPCA = predict(preProc, training[,-ncol(training)])
modelFitPCA = train(factor(training$classe) ~ ., method="rf", data=trainingPCA)
testingPCA = predict(preProc, testing[,-ncol(testing)])
```
Esimate of out of sample error for Random Forest with PCA:
```{r Random Forest with PCA Test, cache=TRUE}
confusionMatrix(factor(testing$classe), predict(modelFitPCA, testingPCA))
```
While overall accuracy stands at 97.55%, results across outcome classes are not 
consistent: Class E sensitivty is 99.66% while Class D is only 92.60%.
Indeed, changing model to operate on predictors without PCA achieves better results:

```{r Random Forest, cache=TRUE}
modelFitRF = train(factor(training$classe) ~ ., method="rf", data=training)
```
Estimate of out of sample error for Random Forest (no PCA):
```{r Random Forest Test, cache=TRUE}
confusionMatrix(factor(testing$classe), predict(modelFitRF, testing))
```
Now, accuracy stands at 99.27% and class outcome statistics improved as well:
the best Class E sensitivity is 100% and the worst Class C sensitivity still
stands high at 97.26%. Each class also shows specificity 99.61% or even as 
hight as 100%. This result is quite satisfactory for this exercise.





